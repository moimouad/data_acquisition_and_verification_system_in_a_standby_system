{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpVz_bKjVl-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import seaborn as sb\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk.corpus \n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "nltk.download('treebank')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86HG6YIrVqzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdnLKSoJVuGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -uq \"drive/My Drive/PFA/PFA.zip\" -d \"drive/My Drive/PFA\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhQIpvmLV54m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_filename = 'drive/My Drive/PFA/test.csv'\n",
        "train_filename = 'drive/My Drive/PFA/train.csv'\n",
        "valid_filename = 'drive/My Drive/PFA/valid.csv'\n",
        "\n",
        "train_news = pd.read_csv(train_filename)\n",
        "test_news = pd.read_csv(test_filename)\n",
        "valid_news = pd.read_csv(valid_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZVlMQ9GWIUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_news.drop(train_news.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
        "test_news.drop(test_news.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
        "valid_news.drop(valid_news.filter(regex=\"Unname\"),axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES0wkHudWMl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data observation\n",
        "def data_obs():\n",
        "    print(\"training dataset size:\")\n",
        "    print(train_news.shape)\n",
        "    print(train_news.head(10))\n",
        "\n",
        "    #below dataset were used for testing and validation purposes\n",
        "    print(test_news.shape)\n",
        "    print(test_news.head(10))\n",
        "    \n",
        "    print(valid_news.shape)\n",
        "    print(valid_news.head(10))\n",
        "\n",
        "#check the data by calling below function\n",
        "data_obs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6fzLjXkWu5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#distribution of classes for prediction\n",
        "def create_distribution(dataFile):\n",
        "    \n",
        "    return sb.countplot(x='label', data=dataFile, palette='hls')\n",
        "    \n",
        "\n",
        "#by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes\n",
        "create_distribution(train_news)\n",
        "create_distribution(test_news)\n",
        "create_distribution(valid_news)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIhht8HZW_nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_qualityCheck():\n",
        "    \n",
        "    print(\"Checking data qualitites...\")\n",
        "    train_news.isnull().sum()\n",
        "    train_news.info()\n",
        "        \n",
        "    print(\"check finished.\")\n",
        "\n",
        "    #below datasets were used to \n",
        "    test_news.isnull().sum()\n",
        "    test_news.info()\n",
        "\n",
        "    valid_news.isnull().sum()\n",
        "    valid_news.info()\n",
        "\n",
        "#run the below function call to see the quality check results\n",
        "\n",
        "data_qualityCheck()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4tttqjdXUND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "eng_stemmer = SnowballStemmer('english')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "#Stemming\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for token in tokens:\n",
        "        stemmed.append(stemmer.stem(token))\n",
        "    return stemmed\n",
        "\n",
        "#process the data\n",
        "def process_data(data,exclude_stopword=True,stem=True):\n",
        "    tokens = [w.lower() for w in data]\n",
        "    tokens_stemmed = tokens\n",
        "    tokens_stemmed = stem_tokens(tokens, eng_stemmer)\n",
        "    tokens_stemmed = [w for w in tokens_stemmed if w not in stopwords ]\n",
        "    return tokens_stemmed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VqjhneLXhAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating ngrams\n",
        "#unigram \n",
        "def create_unigram(words):\n",
        "    assert type(words) == list\n",
        "    return words\n",
        "\n",
        "#bigram\n",
        "def create_bigrams(words):\n",
        "    assert type(words) == list\n",
        "    skip = 0\n",
        "    join_str = \" \"\n",
        "    Len = len(words)\n",
        "    if Len > 1:\n",
        "        lst = []\n",
        "        for i in range(Len-1):\n",
        "            for k in range(1,skip+2):\n",
        "                if i+k < Len:\n",
        "                    lst.append(join_str.join([words[i],words[i+k]]))\n",
        "    else:\n",
        "        #set it as unigram\n",
        "        lst = create_unigram(words)\n",
        "    return lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4X6nkFlXqbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymMcr-owXyVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (len(train_news['label'])) :\n",
        "  if (train_news['label'][i]==\"real\"):\n",
        "    train_news['label'][i]=\"true\"\n",
        "  else:\n",
        "    train_news['label'][i]=\"false\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3l_1S0zYKVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (len(test_news['label'])) :\n",
        "  if (test_news['label'][i]==\"real\"):\n",
        "    test_news['label'][i]=\"true\"\n",
        "  else:\n",
        "    test_news['label'][i]=\"false\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_wurbphbHrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (len(valid_news['label'])) :\n",
        "  if (valid_news['label'][i]==\"real\"):\n",
        "    valid_news['label'][i]=\"true\"\n",
        "  else:\n",
        "    valid_news['label'][i]=\"false\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdNDRGwRbz1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we will start with simple bag of words technique \n",
        "#creating feature vector - document term matrix\n",
        "countV = CountVectorizer()\n",
        "train_count = countV.fit_transform(train_news['text'].values)\n",
        "\n",
        "print(countV)\n",
        "print(train_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmoOOhPPcOmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print training doc term matrix\n",
        "#we have matrix by calling below\n",
        "def get_countVectorizer_stats():\n",
        "    \n",
        "    #vocab size\n",
        "    train_count.shape\n",
        "\n",
        "    #check vocabulary using below command\n",
        "    print(countV.vocabulary_)\n",
        "\n",
        "    #get feature names\n",
        "    print(countV.get_feature_names()[:25])\n",
        "\n",
        "get_countVectorizer_stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCmJ9zMMdTez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidfV = TfidfTransformer()\n",
        "train_tfidf = tfidfV.fit_transform(train_count)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRXROhVAdmUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bag of words - with n-grams\n",
        "\n",
        "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7koNexeY6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#POS Tagging\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        "\n",
        "cutoff = int(.75 * len(tagged_sentences))\n",
        "training_sentences = train_news['text']\n",
        " \n",
        "print(training_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec7hpMVOr0uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKjPCmYAsPaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building classifier using naive bayes \n",
        "nb_pipeline = Pipeline([\n",
        "        ('NBCV',countV),\n",
        "        ('nb_clf',MultinomialNB())])\n",
        "\n",
        "nb_pipeline.fit(train_news['text'],train_news['label'])\n",
        "predicted_nb = nb_pipeline.predict(test_news['text'])\n",
        "np.mean(predicted_nb == test_news['label'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuZT1DGLtv50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building Linear SVM classfier\n",
        "svm_pipeline = Pipeline([\n",
        "        ('svmCV',countV),\n",
        "        ('svm_clf',svm.LinearSVC())\n",
        "        ])\n",
        "\n",
        "svm_pipeline.fit(train_news['text'],train_news['label'])\n",
        "predicted_svm = svm_pipeline.predict(test_news['text'])\n",
        "np.mean(predicted_svm == test_news['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ENej3lxZ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#logistic regression classifier\n",
        "logR_pipeline_ngram = Pipeline([\n",
        "        ('LogR_tfidf',tfidf_ngram),\n",
        "        ('LogR_clf',LogisticRegression(penalty=\"l2\",C=1))\n",
        "        ])\n",
        "\n",
        "logR_pipeline_ngram.fit(train_news['text'],train_news['label'])\n",
        "predicted_LogR_ngram = logR_pipeline_ngram.predict(test_news['text'])\n",
        "np.mean(predicted_LogR_ngram == test_news['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5nXvOk0a8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving model to the disk\n",
        "model_file = 'drive/My Drive/PFA/final_model.sav'\n",
        "pickle.dump(logR_pipeline_ngram,open(model_file,'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eyy18gB0myI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ee9cb4b-ee58-427d-8d8f-fd1f4996dd65"
      },
      "source": [
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "load_model = pickle.load(open('drive/My Drive/PFA/final_model.sav', 'rb'))\n",
        "#function to run for prediction\n",
        "def detecting_fake_news(var):    \n",
        "#retrieving the best model for prediction call\n",
        "    \n",
        "    prediction = load_model.predict([var])\n",
        "    prob = load_model.predict_proba([var])\n",
        "\n",
        "    return (print(\"The given statement is \",prediction[0]),\n",
        "        print(\"The truth probability score is \",prob[0][1]))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbnNeUHH_tNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "74e36cb1-654e-4c3f-a991-e4bd1482e23e"
      },
      "source": [
        "var1 = \"ABUJA (Reuters) - A major Nigerian oil union suspended a nationwide strike on Monday, the same day it began, after a dispute resolution ended with a domestic oil and gas company recalling laid off staff, the union s president said. \"\n",
        "detecting_fake_news(var1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given statement is  true\n",
            "The truth probability score is  0.7750991349698357\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2pzghAS_kxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f164b2bb-2db2-4ae6-e1ff-3e51af3e2cf8"
      },
      "source": [
        "\n",
        "var2 = \"Republicans were just given a leg up over Democrats in this fall s presidential election in the battleground state of North Carolina, and they have a judge put in place by George W. Bush to thank for it.Federal judge Thomas D. Schroeder decided in favor of Republican legislators in court on Monday, letting a controversial voter ID law stay in place despite strong objections from civil rights groups.Research on Voter ID laws have shown that these laws are often a reliable way for Republican conservatives to cut down on voters that often vote for Democrats, especially minorities and young voters.The judge, Thomas D. Schroeder of Federal District Court in Winston-Salem, wrote near the end of his 485-page opinion that  North Carolina has provided legitimate state interests for its voter ID requirement and electoral system. North Carolina s voter identification law requires people to display one of six credentials, such as a driver s license or passport, before casting a ballot. Those who cannot may complete a  reasonable impediment declaration  and cast a provisional ballot.Schroeder was officially put in place on January 8, 2008, at the beginning of George W. Bush s last year in office.The North Carolina law also banned same-day registration, cut down on the amount of days available for early voting, and stops 16 and 17-year-olds from preregistering to vote.An expert testified at the trial that the law was designed in a way to put extra burden on black and Latino voters. Republican legislators and the state s GOP governor Pat McCrory deny the claim.In 2012, a Republican Pennsylvania State House leader bragged that that state s voter ID laws would  allow Governor Romney to win the state of Pennsylvania  (he didn t), while recently a Republican congressman from Wisconsin said voter ID would make the state   which has recently voted for Democrats   competitive in the fall for Republicans.President Obama won North Carolina in 2008 by 0.32% then lost it in 2012 by 2.04%. Polling in March showed the race in North Carolina effectively a toss-up between the Democratic and Republican presidential front runners.Featured image via Flickr \"\n",
        "\n",
        "detecting_fake_news(var2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given statement is  false\n",
            "The truth probability score is  0.36148812168892314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSBZafxmAz3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b776ab71-7e60-4213-a0f9-a5a65394f06f"
      },
      "source": [
        "var = input(\"Please enter the news text you want to verify: \")\n",
        "print(\"You entered: \" + str(var))\n",
        "\n",
        "detecting_fake_news(var)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the news text you want to verify: LOS ANGELES — A precipitous drop in applications for green cards, citizenship and other programs has threatened the solvency of the federal agency that administers the country’s lawful immigration system, prompting it to seek a $1.2 billion cash infusion from Congress as well as fee hikes to stay afloat.  The United States Citizenship and Immigration Services, which relies on the fees that it charges applicants to fund its operations, said that it could run out of money by the summer because the coronavirus pandemic had resulted in far fewer people applying for visas and other benefits.\n",
            "You entered: LOS ANGELES — A precipitous drop in applications for green cards, citizenship and other programs has threatened the solvency of the federal agency that administers the country’s lawful immigration system, prompting it to seek a $1.2 billion cash infusion from Congress as well as fee hikes to stay afloat.  The United States Citizenship and Immigration Services, which relies on the fees that it charges applicants to fund its operations, said that it could run out of money by the summer because the coronavirus pandemic had resulted in far fewer people applying for visas and other benefits.\n",
            "The given statement is  true\n",
            "The truth probability score is  0.6612860090895869\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tuBgdnF0c9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}